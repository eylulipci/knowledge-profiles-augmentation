{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b2a03-c586-4e74-8e35-d70deb63537f",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4d5ff-059f-459b-9bfb-e42878635277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316806e-8bcd-455c-bc05-b0d98b45d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch \n",
    "\n",
    "def get_embeddings(texts, model='bert', token='cls'):\n",
    "\n",
    "    if model == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    else:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    if token == 'cls':\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        return sentence_embeddings\n",
    "    else:\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba6810-62be-4492-8603-8c945dd318c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_for_mistake_and_profile(df, knowledge_profile, mistake_type=None, profile_column='Knowledge Profile', mistake_column='Comments - Eyl√ºl', answer_column='CONTRAPOSITION task'):\n",
    "    \"\"\"\n",
    "    Retrieves example answers from the DataFrame based on a given knowledge profile and optionally a mistake type.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    knowledge_profile (str): The knowledge profile to filter by.\n",
    "    mistake_type (str, optional): The type of mistake to filter by. If None, only knowledge profile is used.\n",
    "    profile_column (str): The column name for the knowledge profile.\n",
    "    mistake_column (str): The column name for the mistake type.\n",
    "    answer_column (str): The column name for the answers.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of answers matching the given criteria.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df[profile_column] == knowledge_profile]\n",
    "    \n",
    "    if mistake_type is not None:\n",
    "        filtered_df = filtered_df[filtered_df[mistake_column].str.contains(mistake_type, case=False, na=False)]\n",
    "\n",
    "    answers = filtered_df[answer_column].tolist()\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57be419-9f87-4753-921a-e7ed91f72ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_txt_files_to_array(directory):\n",
    "    txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    content_array = []\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "            content_array.append(content)\n",
    "\n",
    "    print(f\"Total files read: {len(content_array)}\")\n",
    "    return content_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ad2f7-b13e-49ad-b722-a9bdaa174194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def plot_original_and_generated_kp(knowledge_profile, original_embeddings, generated_data_dir, model='bert', token='cls', csv_path=None):\n",
    "    if csv_path is None:\n",
    "        print(\"CSV path must be provided.\")\n",
    "        return\n",
    "\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    rubric_conditions = [data[f\"rubric{i}\"] == int(k) for i, k in enumerate(knowledge_profile, start=1)]\n",
    "    filtered_data = data.loc[np.logical_and.reduce(rubric_conditions)]\n",
    "\n",
    "    if len(filtered_data) < 100:\n",
    "        print(f\"Not enough samples for knowledge profile {knowledge_profile}.\")\n",
    "        return\n",
    "\n",
    "    selected_samples = filtered_data.sample(n=100, random_state=42)['text'].tolist()\n",
    "\n",
    "    generated_embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    generated_profile_dir = os.path.join(generated_data_dir, knowledge_profile)\n",
    "    if not os.path.exists(generated_profile_dir):\n",
    "        print(f\"No generated data found for knowledge profile: {knowledge_profile}\")\n",
    "        return\n",
    "\n",
    "    # Match samples with mistake directories for labels\n",
    "    for mistake_type in os.listdir(generated_profile_dir):\n",
    "        if \".ipynb_checkpoints\" in mistake_type:\n",
    "            continue\n",
    "\n",
    "        mistake_dir = os.path.join(generated_profile_dir, mistake_type)\n",
    "        if os.path.isdir(mistake_dir):\n",
    "            mistake_texts = read_txt_files_to_array(mistake_dir)\n",
    "\n",
    "            for sample in selected_samples:\n",
    "                if sample in mistake_texts:\n",
    "                    labels.append(mistake_type)\n",
    "                    embeddings = get_embeddings([sample], model, token)\n",
    "                    generated_embeddings.append(embeddings)\n",
    "\n",
    "     # Ensure all selected samples have labels\n",
    "    if len(labels) < len(selected_samples):\n",
    "        print(f\"Warning: Some samples for profile {knowledge_profile} could not be labeled.\")\n",
    "\n",
    "\n",
    "    # Generate colors for plotting based on unique labels\n",
    "    unique_labels = list(set(labels))\n",
    "    print(len(unique_labels))\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_labels) + 2)\n",
    "    colors = {label: cmap(i + 1) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "\n",
    "    # Combine embeddings for PCA\n",
    "    all_embeddings = np.vstack([original_embeddings] + generated_embeddings)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(all_embeddings)\n",
    "\n",
    "    # Plot original data\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(\n",
    "        reduced[:len(original_embeddings), 0],\n",
    "        reduced[:len(original_embeddings), 1],\n",
    "        label='Original Data',\n",
    "        c=cmap(0),\n",
    "        marker='o',\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Plot generated data\n",
    "    start_idx = len(original_embeddings)\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        plt.scatter(\n",
    "            reduced[start_idx + np.array(indices), 0],\n",
    "            reduced[start_idx + np.array(indices), 1],\n",
    "            label=f'Generated ({label})',\n",
    "            c=[colors[label]],\n",
    "            marker='x',\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    plt.title(f'Original and Generated (Simple) Embeddings for Knowledge Profile {knowledge_profile}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "\n",
    "    figure_name = f\"simple_{knowledge_profile}_{model}_{token}.png\"\n",
    "    plt.savefig(os.path.join(\"diversity_figures\", figure_name))\n",
    "\n",
    "    plt.show()\n",
    "    return generated_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558269b-a410-49a0-9587-a1c9ea91c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def compute_similarity(embeddings1, embeddings2=None, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Computes the average pairwise similarity within a single group of embeddings\n",
    "    or between two groups of embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    embeddings1 (np.array): Sentence embeddings for the first group (shape: [n_samples1, embedding_dim])\n",
    "    embeddings2 (np.array, optional): Sentence embeddings for the second group (shape: [n_samples2, embedding_dim])\n",
    "    metric (str): Type of similarity to compute ('cosine' or 'euclidean')\n",
    "    \n",
    "    Returns:\n",
    "    float: The average pairwise similarity within the group or between the two groups.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings if computing cosine similarity\n",
    "    if metric == 'cosine':\n",
    "        embeddings1 = embeddings1 / np.linalg.norm(embeddings1, axis=1, keepdims=True)\n",
    "        if embeddings2 is not None:\n",
    "            embeddings2 = embeddings2 / np.linalg.norm(embeddings2, axis=1, keepdims=True)\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    # If a second group is provided, compute between-group similarity\n",
    "    if embeddings2 is not None:\n",
    "        for emb1 in embeddings1:\n",
    "            for emb2 in embeddings2:\n",
    "                if metric == 'cosine':\n",
    "                    similarity = np.dot(emb1, emb2)\n",
    "                elif metric == 'euclidean':\n",
    "                    similarity = np.linalg.norm(emb1 - emb2)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid metric. Use 'cosine' or 'euclidean'.\")\n",
    "                similarities.append(similarity)\n",
    "    else:\n",
    "        # Compute within-group similarity\n",
    "        for emb1, emb2 in combinations(embeddings1, 2):\n",
    "            if metric == 'cosine':\n",
    "                similarity = np.dot(emb1, emb2)\n",
    "            elif metric == 'euclidean':\n",
    "                similarity = np.linalg.norm(emb1 - emb2)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid metric. Use 'cosine' or 'euclidean'.\")\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1516ac-63c7-4448-b44c-c85476b55d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(metrics, target_knowledge_profile, method_name='Simple'):\n",
    "    \"\"\"\n",
    "    Generates a LaTeX table from computed metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Dictionary with metrics.\n",
    "            Example structure:\n",
    "                {\n",
    "                    \"cosine_within_original\": 0.9678,\n",
    "                    \"euclidean_within_original\": 3.1954,\n",
    "                    \"cosine_within_generated\": 0.9863,\n",
    "                    \"euclidean_within_generated\": 2.1517,\n",
    "                    \"cosine_between\": 0.9718,\n",
    "                    \"euclidean_between\": 3.0448,\n",
    "                    \"energy_between\": 0.3936\n",
    "                }\n",
    "        target_knowledge_profile (str): Knowledge profile for the data.\n",
    "        method_name (str): Name of the method for the generated data.\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX table as a string.\n",
    "    \"\"\"\n",
    "    latex_table = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\begin{{tabular}}{{cccc}}\n",
    "\\\\hline\n",
    "\\\\textbf{{Dataset}}              & \\\\textbf{{Cosine}} & \\\\textbf{{Euclidean}} & \\\\textbf{{Energy}} \\\\\\\\ \\\\hline\n",
    "Original                      & {metrics['cosine_within_original']:.4f}          & {metrics['euclidean_within_original']:.4f}             & -               \\\\\\\\\n",
    "Generated ({method_name})            & {metrics['cosine_within_generated']:.4f}          & {metrics['euclidean_within_generated']:.4f}             & -               \\\\\\\\\n",
    "Original - Generated ({method_name}) & {metrics['cosine_between']:.4f}          & {metrics['euclidean_between']:.4f}             & {metrics['energy_between']:.4f}          \\\\\\\\ \\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\caption{{{target_knowledge_profile} {method_name}}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d83fa-42a4-4049-aab9-4901dbc5bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# Define the energy distance loss\n",
    "# \"energy\" type is used to compute the Energy Distance (also known as C2 distance)\n",
    "energy_distance_nd = SamplesLoss(loss=\"energy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9cc65-2d8a-4c75-885b-813d70aee621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path/to/original/data'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "df = pd.read_excel(dataset_path, sheet_name='contraposition')\n",
    "\n",
    "# rubrics of the original dataset\n",
    "rubrics = [\n",
    "    'Statement of what should be proven: A proof by contraposition of an implication consists in showing that if x rational, then x^2 is rational. ',\n",
    "    'Correct assumption: x is rational [Assumption] ',\n",
    "    'Correct proof reasoning',\n",
    "    'Proof conclusion: By contraposition, if x^2 is irrational, then x is irrational.'\n",
    "]\n",
    "\n",
    "# knowledge profile column\n",
    "df['Knowledge Profile'] = df[rubrics].astype(str).agg(''.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f34780-d0eb-4b86-9994-5bcadb33baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_dir = \"generation/generation_simple/correct_responses/temp_1\"\n",
    "csv_path = \"generated_datasets/simple_mixed_data_500.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26cf9e-6d41-4b7c-99ba-dd184eb553cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert', 'distilbert']\n",
    "tokens = ['cls', 'mean']\n",
    "\n",
    "generated_embeddings_dict = {model: {token: [] for token in tokens} for model in models}\n",
    "original_embeddings_dict = {model: {token: [] for token in tokens} for model in models}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc3093-1c69-4dd6-91a8-a53b6e4a3c83",
   "metadata": {},
   "source": [
    "## 0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44188f-6e0d-4503-8140-2813446e0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "target_knowledge_profile = '0000'\n",
    "\n",
    "print(f'Results for {model} - {token} - {target_knowledge_profile}')\n",
    "# retrieve all original data for this knowledge\n",
    "original_kp_data = get_answers_for_mistake_and_profile(df, target_knowledge_profile)\n",
    "# Compute embeddings for the original data\n",
    "embeddings_original_kp_data = get_embeddings(original_kp_data, model, token)\n",
    "generated_embeddings = plot_original_and_generated_kp(target_knowledge_profile, embeddings_original_kp_data, generated_data_dir, model, token, csv_path)        \n",
    "generated_embeddings = torch.vstack(generated_embeddings)\n",
    "\n",
    "generated_embeddings_dict[model][token].append(generated_embeddings)\n",
    "original_embeddings_dict[model][token].append(embeddings_original_kp_data)\n",
    "\n",
    "metrics = {\n",
    "        \"cosine_within_original\": compute_similarity(embeddings_original_kp_data),\n",
    "        \"euclidean_within_original\": compute_similarity(embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"cosine_within_generated\": compute_similarity(generated_embeddings),\n",
    "        \"euclidean_within_generated\": compute_similarity(generated_embeddings, metric=\"euclidean\"),\n",
    "        \"cosine_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data),\n",
    "        \"euclidean_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"energy_between\": energy_distance_nd(generated_embeddings, embeddings_original_kp_data),\n",
    "    }\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_{target_knowledge_profile}_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "#### METRICS #####\n",
    "# Between samples\n",
    "print(f'Cosine similarity within generated: {compute_similarity(generated_embeddings)}')\n",
    "print(f'Euclidean distance within generated: {compute_similarity(generated_embeddings, metric=\"euclidean\")}')\n",
    "\n",
    "print(f'Cosine similarity within original: {compute_similarity(embeddings_original_kp_data)}')\n",
    "print(f'Euclidean distance within original: {compute_similarity(embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "\n",
    "# between ds\n",
    "print(f'Cosine similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data)}')\n",
    "print(f'Euclidean similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "print(f'Energy distance between original - generated: {energy_distance_nd(generated_embeddings, embeddings_original_kp_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec0453-c4f3-4679-b6c0-7e8c64cf2784",
   "metadata": {},
   "source": [
    "## 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992fa30-214a-4dd0-aa91-3be4bdc41adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "target_knowledge_profile = '1000'\n",
    "\n",
    "print(f'Results for {model} - {token} - {target_knowledge_profile}')\n",
    "# retrieve all original data for this knowledge\n",
    "original_kp_data = get_answers_for_mistake_and_profile(df, target_knowledge_profile)\n",
    "# Compute embeddings for the original data\n",
    "embeddings_original_kp_data = get_embeddings(original_kp_data, model, token)\n",
    "generated_embeddings = plot_original_and_generated_kp(target_knowledge_profile, embeddings_original_kp_data, generated_data_dir, model, token, csv_path)        \n",
    "generated_embeddings = torch.vstack(generated_embeddings)\n",
    "\n",
    "generated_embeddings_dict[model][token].append(generated_embeddings)\n",
    "original_embeddings_dict[model][token].append(embeddings_original_kp_data)\n",
    "\n",
    "metrics = {\n",
    "        \"cosine_within_original\": compute_similarity(embeddings_original_kp_data),\n",
    "        \"euclidean_within_original\": compute_similarity(embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"cosine_within_generated\": compute_similarity(generated_embeddings),\n",
    "        \"euclidean_within_generated\": compute_similarity(generated_embeddings, metric=\"euclidean\"),\n",
    "        \"cosine_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data),\n",
    "        \"euclidean_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"energy_between\": energy_distance_nd(generated_embeddings, embeddings_original_kp_data),\n",
    "    }\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_{target_knowledge_profile}_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "#### METRICS #####\n",
    "# Between samples\n",
    "print(f'Cosine similarity within generated: {compute_similarity(generated_embeddings)}')\n",
    "print(f'Euclidean distance within generated: {compute_similarity(generated_embeddings, metric=\"euclidean\")}')\n",
    "\n",
    "print(f'Cosine similarity within original: {compute_similarity(embeddings_original_kp_data)}')\n",
    "print(f'Euclidean distance within original: {compute_similarity(embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "\n",
    "# between ds\n",
    "print(f'Cosine similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data)}')\n",
    "print(f'Euclidean similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "print(f'Energy distance between original - generated: {energy_distance_nd(generated_embeddings, embeddings_original_kp_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e280680-e48b-46fd-8e5a-47efd877e9a4",
   "metadata": {},
   "source": [
    "## 1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf921c2-ee15-4ed7-a94b-2417725e1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "target_knowledge_profile = '1110'\n",
    "\n",
    "print(f'Results for {model} - {token} - {target_knowledge_profile}')\n",
    "# retrieve all original data for this knowledge\n",
    "original_kp_data = get_answers_for_mistake_and_profile(df, target_knowledge_profile)\n",
    "# Compute embeddings for the original data\n",
    "embeddings_original_kp_data = get_embeddings(original_kp_data, model, token)\n",
    "generated_embeddings = plot_original_and_generated_kp(target_knowledge_profile, embeddings_original_kp_data, generated_data_dir, model, token, csv_path)        \n",
    "generated_embeddings = torch.vstack(generated_embeddings)\n",
    "\n",
    "generated_embeddings_dict[model][token].append(generated_embeddings)\n",
    "original_embeddings_dict[model][token].append(embeddings_original_kp_data)\n",
    "\n",
    "metrics = {\n",
    "        \"cosine_within_original\": compute_similarity(embeddings_original_kp_data),\n",
    "        \"euclidean_within_original\": compute_similarity(embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"cosine_within_generated\": compute_similarity(generated_embeddings),\n",
    "        \"euclidean_within_generated\": compute_similarity(generated_embeddings, metric=\"euclidean\"),\n",
    "        \"cosine_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data),\n",
    "        \"euclidean_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"energy_between\": energy_distance_nd(generated_embeddings, embeddings_original_kp_data),\n",
    "    }\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_{target_knowledge_profile}_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "#### METRICS #####\n",
    "# Between samples\n",
    "print(f'Cosine similarity within generated: {compute_similarity(generated_embeddings)}')\n",
    "print(f'Euclidean distance within generated: {compute_similarity(generated_embeddings, metric=\"euclidean\")}')\n",
    "\n",
    "print(f'Cosine similarity within original: {compute_similarity(embeddings_original_kp_data)}')\n",
    "print(f'Euclidean distance within original: {compute_similarity(embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "\n",
    "# between ds\n",
    "print(f'Cosine similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data)}')\n",
    "print(f'Euclidean similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "print(f'Energy distance between original - generated: {energy_distance_nd(generated_embeddings, embeddings_original_kp_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd8305-b577-410f-a96f-19bccf882387",
   "metadata": {},
   "source": [
    "## 0110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f357e34-9067-4e89-87b0-75c88f5fbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "target_knowledge_profile = '0110'\n",
    "\n",
    "print(f'Results for {model} - {token} - {target_knowledge_profile}')\n",
    "# retrieve all original data for this knowledge\n",
    "original_kp_data = get_answers_for_mistake_and_profile(df, target_knowledge_profile)\n",
    "# Compute embeddings for the original data\n",
    "embeddings_original_kp_data = get_embeddings(original_kp_data, model, token)\n",
    "generated_embeddings = plot_original_and_generated_kp(target_knowledge_profile, embeddings_original_kp_data, generated_data_dir, model, token, csv_path)        \n",
    "generated_embeddings = torch.vstack(generated_embeddings)\n",
    "\n",
    "generated_embeddings_dict[model][token].append(generated_embeddings)\n",
    "original_embeddings_dict[model][token].append(embeddings_original_kp_data)\n",
    "\n",
    "metrics = {\n",
    "        \"cosine_within_original\": compute_similarity(embeddings_original_kp_data),\n",
    "        \"euclidean_within_original\": compute_similarity(embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"cosine_within_generated\": compute_similarity(generated_embeddings),\n",
    "        \"euclidean_within_generated\": compute_similarity(generated_embeddings, metric=\"euclidean\"),\n",
    "        \"cosine_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data),\n",
    "        \"euclidean_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"energy_between\": energy_distance_nd(generated_embeddings, embeddings_original_kp_data),\n",
    "    }\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_{target_knowledge_profile}_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "#### METRICS #####\n",
    "# Between samples\n",
    "print(f'Cosine similarity within generated: {compute_similarity(generated_embeddings)}')\n",
    "print(f'Euclidean distance within generated: {compute_similarity(generated_embeddings, metric=\"euclidean\")}')\n",
    "\n",
    "print(f'Cosine similarity within original: {compute_similarity(embeddings_original_kp_data)}')\n",
    "print(f'Euclidean distance within original: {compute_similarity(embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "\n",
    "# between ds\n",
    "print(f'Cosine similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data)}')\n",
    "print(f'Euclidean similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "print(f'Energy distance between original - generated: {energy_distance_nd(generated_embeddings, embeddings_original_kp_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bedfe0-5a47-4394-b2c3-eb3ea1911f50",
   "metadata": {},
   "source": [
    "## 0111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd752d0-4822-46d9-ba18-9fc7c0732978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "target_knowledge_profile = '0111'\n",
    "\n",
    "print(f'Results for {model} - {token} - {target_knowledge_profile}')\n",
    "# retrieve all original data for this knowledge\n",
    "original_kp_data = get_answers_for_mistake_and_profile(df, target_knowledge_profile)\n",
    "# Compute embeddings for the original data\n",
    "embeddings_original_kp_data = get_embeddings(original_kp_data, model, token)\n",
    "generated_embeddings = plot_original_and_generated_kp(target_knowledge_profile, embeddings_original_kp_data, generated_data_dir, model, token, csv_path)        \n",
    "generated_embeddings = torch.vstack(generated_embeddings)\n",
    "\n",
    "generated_embeddings_dict[model][token].append(generated_embeddings)\n",
    "original_embeddings_dict[model][token].append(embeddings_original_kp_data)\n",
    "\n",
    "metrics = {\n",
    "        \"cosine_within_original\": compute_similarity(embeddings_original_kp_data),\n",
    "        \"euclidean_within_original\": compute_similarity(embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"cosine_within_generated\": compute_similarity(generated_embeddings),\n",
    "        \"euclidean_within_generated\": compute_similarity(generated_embeddings, metric=\"euclidean\"),\n",
    "        \"cosine_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data),\n",
    "        \"euclidean_between\": compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\"),\n",
    "        \"energy_between\": energy_distance_nd(generated_embeddings, embeddings_original_kp_data),\n",
    "    }\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_{target_knowledge_profile}_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to {output_file}\")\n",
    "\n",
    "#### METRICS #####\n",
    "# Between samples\n",
    "print(f'Cosine similarity within generated: {compute_similarity(generated_embeddings)}')\n",
    "print(f'Euclidean distance within generated: {compute_similarity(generated_embeddings, metric=\"euclidean\")}')\n",
    "\n",
    "print(f'Cosine similarity within original: {compute_similarity(embeddings_original_kp_data)}')\n",
    "print(f'Euclidean distance within original: {compute_similarity(embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "\n",
    "# between ds\n",
    "print(f'Cosine similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data)}')\n",
    "print(f'Euclidean similarity between original - generated: {compute_similarity(generated_embeddings, embeddings_original_kp_data, metric=\"euclidean\")}')\n",
    "print(f'Energy distance between original - generated: {energy_distance_nd(generated_embeddings, embeddings_original_kp_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cc128-d843-400d-9189-93081827df2a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08940a65-1f3d-4086-9ec1-380aa261b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "token = 'cls'\n",
    "\n",
    "print(f\"### {model} - {token} ###\") \n",
    "generated = torch.vstack(generated_embeddings_dict[model][token])\n",
    "original = torch.vstack(original_embeddings_dict[model][token])\n",
    "\n",
    "# Combine embeddings for PCA\n",
    "all_embeddings = torch.cat([original, generated], dim=0).numpy()\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# Split the reduced embeddings back\n",
    "original_reduced = reduced[:len(original)]\n",
    "generated_reduced = reduced[len(original):]\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab10', 4)\n",
    "# Plot original and generated data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(\n",
    "    original_reduced[:, 0],\n",
    "    original_reduced[:, 1],\n",
    "    label='Original',\n",
    "    c=cmap(0),\n",
    "    marker='o',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    generated_reduced[:, 0],\n",
    "    generated_reduced[:, 1],\n",
    "    label='Generated',\n",
    "    c=cmap(1),\n",
    "    marker='x',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title(f'Original and Generated (Simple) Embeddings (Dataset)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "figure_name = f\"simple_dataset.png\"\n",
    "os.makedirs(\"diversity_figures\", exist_ok=True)\n",
    "plt.savefig(os.path.join(\"diversity_figures\", figure_name))\n",
    "print(f\"Figure saved as {figure_name}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Metric computation\n",
    "#generated_embeddings = torch.vstack(generated_embeddings_dict[model][token])\n",
    "#original_embeddings = torch.tensor(original_embeddings_dict[model][token])\n",
    "\n",
    "metrics = {\n",
    "    \"cosine_within_original\": compute_similarity(original),\n",
    "    \"euclidean_within_original\": compute_similarity(original, metric=\"euclidean\"),\n",
    "    \"cosine_within_generated\": compute_similarity(generated),\n",
    "    \"euclidean_within_generated\": compute_similarity(generated, metric=\"euclidean\"),\n",
    "    \"cosine_between\": compute_similarity(generated, original),\n",
    "    \"euclidean_between\": compute_similarity(generated, original, metric=\"euclidean\"),\n",
    "    \"energy_between\": energy_distance_nd(generated, original),\n",
    "}\n",
    "\n",
    "latex_table = generate_latex_table(metrics, target_knowledge_profile)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_dir = \"./latex_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"simple_ds_div_table.tex\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a691a-ee90-4cf5-a434-9ad41bf7cd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
